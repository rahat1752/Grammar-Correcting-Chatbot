{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Grammer Correction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gramformer",
      "language": "python",
      "name": "gramformer"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M7j22ccU10G",
        "outputId": "75c66e6f-d152-4ab7-a0ea-9c93f76b762a"
      },
      "source": [
        "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
            "Requirement already satisfied: torch==1.8.1+cu111 in /usr/local/lib/python3.7/dist-packages (1.8.1+cu111)\n",
            "Requirement already satisfied: torchvision==0.9.1+cu111 in /usr/local/lib/python3.7/dist-packages (0.9.1+cu111)\n",
            "Requirement already satisfied: torchaudio===0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cu111) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuRiAr-bjFw6",
        "outputId": "afc56e55-e4c0-4a6b-dbe4-8fc4cb596a1d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk8IX_ipjMW9",
        "outputId": "f4dfdaba-c92c-4f0c-aa85-62485c9778c3"
      },
      "source": [
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ln: failed to create symbolic link '/mydrive/My Drive': File exists\n",
            " 1707008\n",
            " 1707026\n",
            " 1707026.pdf\n",
            " 1707026.zip\n",
            " 1707040\n",
            " 1707059\n",
            " 1e93121c-7ed4-4f3b-ac48-93cd4d673968.jpg\n",
            "'2.1 math'\n",
            " 3.1\n",
            " 3.2\n",
            "'Azhar Sir CT'\n",
            "'Bandarban 2021'\n",
            " Classroom\n",
            "'Colab Notebooks'\n",
            "'Compiler Project'\n",
            "'CSE 2k17 Signatures'\n",
            "'CSE 3212'\n",
            "'CSE 3212 Compiler Design Lab Project Reports 15 June.zip'\n",
            "'CSE 3212 Project Report'\n",
            " dataset\n",
            "'Family Get-together'\n",
            "'Fish n Chips'\n",
            "'Getting started.pdf'\n",
            " HR\n",
            "'Hum Assignments'\n",
            " IMG_20210101_101719.jpg\n",
            " IMG_20210101_170308.jpg\n",
            " Maruf\n",
            "'MBA Thesis Works'\n",
            "'My Drive'\n",
            "\"Nazia Ma'am Assignment\"\n",
            " NID.pdf\n",
            " NormalReplyBot\n",
            "'Paper_1 (1).docx'\n",
            " Paper_1.docx\n",
            " Paper_2.docx\n",
            "'Rokib Sir (17-05).zip'\n",
            "'Rokib Sir Assignment'\n",
            "'saaint martin tour'\n",
            "\"Sabab Vaiya's Engagement\"\n",
            " Screenrecorder-2021-08-03-00-40-24-682.mp4\n",
            "'season 3'\n",
            " test\n",
            "'ToC CT'\n",
            " WP\n",
            " yolov4\n",
            "'পেদা টিং টিং'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qWIUttfU10M",
        "outputId": "b4144a75-8bf6-44f8-b203-15b5bffc3a68"
      },
      "source": [
        "!pip3 install pip==20.1.1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip==20.1.1 in /usr/local/lib/python3.7/dist-packages (20.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH98g1dVbWee",
        "outputId": "5897d857-dbbb-419d-b8bf-a2cbf378343a"
      },
      "source": [
        "!pip3 install git+https://github.com/PrithivirajDamodaran/Gramformer.git"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
            "  Cloning https://github.com/PrithivirajDamodaran/Gramformer.git to /tmp/pip-req-build-bmfj8pru\n",
            "  Running command git clone -q https://github.com/PrithivirajDamodaran/Gramformer.git /tmp/pip-req-build-bmfj8pru\n",
            "Requirement already satisfied (use --upgrade to upgrade): gramformer==1.0 from git+https://github.com/PrithivirajDamodaran/Gramformer.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from gramformer==1.0) (4.12.3)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from gramformer==1.0) (0.1.95)\n",
            "Requirement already satisfied: python-Levenshtein==0.12.2 in /usr/local/lib/python3.7/dist-packages (from gramformer==1.0) (0.12.2)\n",
            "Requirement already satisfied: fuzzywuzzy==0.18.0 in /usr/local/lib/python3.7/dist-packages (from gramformer==1.0) (0.18.0)\n",
            "Requirement already satisfied: tokenizers==0.10.2 in /usr/local/lib/python3.7/dist-packages (from gramformer==1.0) (0.10.2)\n",
            "Requirement already satisfied: fsspec==2021.5.0 in /usr/local/lib/python3.7/dist-packages (from gramformer==1.0) (2021.5.0)\n",
            "Requirement already satisfied: lm-scorer==0.4.2 in /usr/local/lib/python3.7/dist-packages (from gramformer==1.0) (0.4.2)\n",
            "Requirement already satisfied: errant in /usr/local/lib/python3.7/dist-packages (from gramformer==1.0) (2.3.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (0.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->gramformer==1.0) (2019.12.20)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein==0.12.2->gramformer==1.0) (57.4.0)\n",
            "Requirement already satisfied: pip>=20.0.0 in /usr/local/lib/python3.7/dist-packages (from lm-scorer==0.4.2->gramformer==1.0) (20.1.1)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from lm-scorer==0.4.2->gramformer==1.0) (1.8.1+cu111)\n",
            "Requirement already satisfied: spacy<3,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from errant->gramformer==1.0) (2.2.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->gramformer==1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->gramformer==1.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->gramformer==1.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers->gramformer==1.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->gramformer==1.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->gramformer==1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->gramformer==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->gramformer==1.0) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->gramformer==1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->gramformer==1.0) (2.4.7)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (2.0.5)\n",
            "Building wheels for collected packages: gramformer\n",
            "  Building wheel for gramformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gramformer: filename=gramformer-1.0-py3-none-any.whl size=4501 sha256=d636462c4fc05e1245c8ac3e4317c60deed6a264648ad4ccdae0a4d5580cbabc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-75wu0ec2/wheels/c1/f6/04/a7ceda8a8c1d494a3523957d61eb6dfe428568b1af0b9d508c\n",
            "Successfully built gramformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Xkz2q5U10N"
      },
      "source": [
        "from gramformer import Gramformer"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srsmhZsfcNwI",
        "outputId": "85eab9d3-f372-49c2-e130-3284f5288fe1"
      },
      "source": [
        "from gramformer import Gramformer\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1212)\n",
        "\n",
        "\n",
        "gf = Gramformer(models = 1, use_gpu=False) # 1=corrector, 2=detector\n",
        "\n",
        "influent_sentences = [\n",
        "    \"He are moving here.\",\n",
        "    \"I am doing fine. How is you?\",\n",
        "    \"How is they?\",\n",
        "    \"Matt like fish\",\n",
        "    \"the collection of letters was original used by the ancient Romans\",\n",
        "    \"We enjoys horror movies\",\n",
        "    \"Anna and Mike is going skiing\",\n",
        "    \"I walk to the store and I bought milk\",\n",
        "    \" We all eat the fish and then made dessert\",\n",
        "    \"I will eat fish for dinner and drink milk\",\n",
        "    \"what be the reason for everyone leave the company\",\n",
        "]   \n",
        "\n",
        "for influent_sentence in influent_sentences:\n",
        "    corrected_sentences = gf.correct(influent_sentence, max_candidates=1)\n",
        "    print(\"[Input] \", influent_sentence)\n",
        "    for corrected_sentence in corrected_sentences:\n",
        "      print(\"[Correction] \",corrected_sentence)\n",
        "    print(\"-\" *100)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gramformer] Grammar error correct/highlight model loaded..\n",
            "[Input]  He are moving here.\n",
            "[Correction]  ('He is moving here.', -31.02849578857422)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  I am doing fine. How is you?\n",
            "[Correction]  ('I am doing fine, how are you?', -37.671016693115234)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  How is they?\n",
            "[Correction]  ('How are they?', -24.648284912109375)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Matt like fish\n",
            "[Correction]  ('Matt likes fish.', -33.768829345703125)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  the collection of letters was original used by the ancient Romans\n",
            "[Correction]  ('the collection of letters was original used by the ancient Romans', -63.627716064453125)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  We enjoys horror movies\n",
            "[Correction]  ('We enjoy horror movies.', -31.77159881591797)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  Anna and Mike is going skiing\n",
            "[Correction]  ('Anna and Mike are going skiing.', -42.5970458984375)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  I walk to the store and I bought milk\n",
            "[Correction]  ('I ran to the store and I bought milk.', -47.339088439941406)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]   We all eat the fish and then made dessert\n",
            "[Correction]  ('We all ate the fish and then made dessert.', -54.92893981933594)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  I will eat fish for dinner and drink milk\n",
            "[Correction]  ('I will eat fish for dinner and drink milk.', -44.48554229736328)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[Input]  what be the reason for everyone leave the company\n",
            "[Correction]  ('what is the reason for everyone leaving the company?', -45.09092712402344)\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS4txu9WXNro",
        "outputId": "55375939-5dac-4dec-f4ab-c7194d91d5e8"
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r\"i'm\", \"i am\", txt)\n",
        "    txt = re.sub(r\"it's\", \"it is\", txt)\n",
        "    txt = re.sub(r\"he's\", \"he is\", txt)\n",
        "    txt = re.sub(r\"she's\", \"she is\", txt)\n",
        "    txt = re.sub(r\"that's\", \"that is\", txt)\n",
        "    txt = re.sub(r\"what's\", \"what is\", txt)\n",
        "    txt = re.sub(r\"where's\", \"where is\", txt)\n",
        "    txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
        "    txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
        "    txt = re.sub(r\"\\'re\", \" are\", txt)\n",
        "    txt = re.sub(r\"\\'d\", \" would\", txt)\n",
        "    txt = re.sub(r\"won't\", \"will not\", txt)\n",
        "    txt = re.sub(r\"can't\", \"can not\", txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "\n",
        "def create_training_data():\n",
        "    data_path = r'/mydrive/Maruf/Final_Everything/Arcadia/human_text.txt'\n",
        "    data_path2 = r'/mydrive/Maruf/Final_Everything/Arcadia/robot_text.txt'\n",
        "    # Defining lines as a list of each line\n",
        "    with open(data_path, \"r\", encoding='utf-8') as f:\n",
        "        lines = f.read().split('\\n')\n",
        "    with open(data_path2, \"r\", encoding='utf-8') as f:\n",
        "        lines2 = f.read().split('\\n')\n",
        "    lines = [re.sub(r\"\\[\\w+\\]\", 'hi', line) for line in lines]\n",
        "    lines = [\" \".join(re.findall(r\"\\w+\", line)) for line in lines]\n",
        "    lines2 = [re.sub(r\"\\[\\w+\\]\", '', line) for line in lines2]\n",
        "    lines2 = [\" \".join(re.findall(r\"\\w+\", line)) for line in lines2]\n",
        "\n",
        "    encoder_input_data = []\n",
        "    decoder_input_data = []\n",
        "    decoder_output_data = []\n",
        "\n",
        "\n",
        "    for i in range(len(lines)):\n",
        "        encoder_input_data.append(clean_text(lines[i]))\n",
        "        decoder_input_data.append('<sos> ' + clean_text(lines2[i]))\n",
        "        decoder_output_data.append(clean_text(lines2[i]) + ' <eos>')\n",
        "    return encoder_input_data, decoder_input_data, decoder_output_data\n",
        "\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "\n",
        "VOCAB_SIZE = 5000\n",
        "MAXLEN = 40\n",
        "EPOCHS = 150\n",
        "BATCH_SIZE = 1024\n",
        "VERBOSE = 1\n",
        "SAVE_AT = 50\n",
        "LEARNING_RATE = 0.01\n",
        "LOSS = 'sparse_categorical_crossentropy'\n",
        "a,b,c=create_training_data()\n",
        "c[:5]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi there how are you <eos>',\n",
              " 'here is afternoon <eos>',\n",
              " 'my name is rdany but you can call me dany the r means robot i hope we can be virtual friends <eos>',\n",
              " 'i have many but not enough to fully understand humans beings <eos>',\n",
              " 'i ve talked with 143 users counting 7294 lines of text <eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruDz9ZhZXVkA"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Embedding\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "\n",
        "def seq2seq():\n",
        "    encoder_inputs= Input(shape=(40,))\n",
        "    encoder_embedding = Embedding(VOCAB_SIZE, 100, input_length=MAXLEN)\n",
        "\n",
        "    decoder_embedding = Embedding(VOCAB_SIZE, 100, input_length=MAXLEN)\n",
        "    encoder_embeddings = encoder_embedding(encoder_inputs)\n",
        "    encoder_lstm=LSTM(256, return_state=True, kernel_regularizer=l2(0.0000001), activity_regularizer=l2(0.0000001))\n",
        "    LSTM_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
        "\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(40,), name='decoder_inputs')\n",
        "    decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name='decoder_lstm', kernel_regularizer=l2(0.0000001), activity_regularizer=l2(0.0000001))\n",
        "    decoder_embeddings = decoder_embedding(decoder_inputs)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embeddings,\n",
        "                                         initial_state=encoder_states)\n",
        "\n",
        "\n",
        "    decoder_dense = Dense(5000, activation='softmax', name='decoder_dense')\n",
        "\n",
        "\n",
        "\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    print(decoder_outputs)\n",
        "    seq2seq = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='model_encoder_training')\n",
        "\n",
        "    return seq2seq"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS4ajXZWXdTF"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, vocab_size=10000, maxlen=40, padding='post'):\n",
        "\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<unk>\",\n",
        "                                                               filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ', split=' ')\n",
        "        self.maxlen = maxlen\n",
        "        self.padding = padding\n",
        "\n",
        "    def tokenize_and_pad_training_data(self, encoder_input_data, decoder_input_data, decoder_output_data):\n",
        "        text_corpus = encoder_input_data + decoder_input_data + decoder_output_data\n",
        "        self.tokenizer.fit_on_texts(text_corpus)\n",
        "\n",
        "        self.tokenizer.word_index['<pad>'] = 0\n",
        "        self.tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "        encoder_input_data_tokenized = self.tokenizer.texts_to_sequences(encoder_input_data)\n",
        "        decoder_input_data_tokenized = self.tokenizer.texts_to_sequences(decoder_input_data)\n",
        "        decoder_output_data_tokenized = self.tokenizer.texts_to_sequences(decoder_output_data)\n",
        "\n",
        "        encoder_input_data_padded = tf.keras.preprocessing.sequence.pad_sequences(encoder_input_data_tokenized,\n",
        "                                                                                  padding=self.padding,\n",
        "                                                                                  maxlen=self.maxlen)\n",
        "        decoder_input_data_padded = tf.keras.preprocessing.sequence.pad_sequences(decoder_input_data_tokenized,\n",
        "                                                                                  padding=self.padding,\n",
        "                                                                                  maxlen=self.maxlen)\n",
        "        decoder_output_data_padded = tf.keras.preprocessing.sequence.pad_sequences(decoder_output_data_tokenized,\n",
        "                                                                                   padding=self.padding,\n",
        "                                                                                   maxlen=self.maxlen)\n",
        "\n",
        "        return encoder_input_data_padded, decoder_input_data_padded, decoder_output_data_padded\n",
        "\n",
        "    def decode_sequence(self, encoded_text):\n",
        "        lst = []\n",
        "        for i in encoded_text:\n",
        "            lst.append(self.tokenizer.index_word[i])\n",
        "        return ' '.join(lst)\n",
        "\n",
        "    def tokenize_sequence(self, sequence):\n",
        "        tokenized_sequence = []\n",
        "        sequence = sequence.lower()\n",
        "        sequence = sequence.strip()\n",
        "        sequence = re.sub(r'[^\\w\\s]', '', sequence)\n",
        "        for i in sequence.split(' '):\n",
        "            try:\n",
        "                tokenized_sequence.append(self.tokenizer.word_index[i])\n",
        "            except:\n",
        "                tokenized_sequence.append(self.tokenizer.word_index['well'])\n",
        "        if len(tokenized_sequence) > 40:\n",
        "            tokenized_sequence = tokenized_sequence[:40]\n",
        "        elif len(tokenized_sequence) == 40:\n",
        "            tokenized_sequence = tokenized_sequence\n",
        "        else:\n",
        "            length = len(tokenized_sequence)\n",
        "        for i in range(40 - length):\n",
        "            tokenized_sequence.append(0)\n",
        "        return tokenized_sequence\n",
        "\n",
        "    def save_tokenizer(self, name):\n",
        "        with open(f'{name}.pickle', 'wb') as handle:\n",
        "            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def load_tokenizer(self, path):\n",
        "        with open(path, 'rb') as handle:\n",
        "            tokenizer = pickle.load(handle)\n",
        "            self.tokenizer = tokenizer\n",
        "        return tokenizer"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG4HP9UcXnvB"
      },
      "source": [
        "from math import log\n",
        "import numpy as np\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import LSTM, Embedding\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++ seq2seq model to refere layers with their names ++++++++++++++++++++++++++++++++\n",
        "encoder_inputs = Input(shape=(40,))\n",
        "encoder_embedding = Embedding(VOCAB_SIZE, 100, input_length=MAXLEN)\n",
        "\n",
        "decoder_embedding = Embedding(VOCAB_SIZE, 100, input_length=MAXLEN)\n",
        "encoder_embeddings = encoder_embedding(encoder_inputs)\n",
        "encoder_lstm = LSTM(256, return_state=True, kernel_regularizer=l2(0.0000001), activity_regularizer=l2(0.0000001))\n",
        "LSTM_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(40,), name='decoder_inputs')\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name='decoder_lstm',\n",
        "                    kernel_regularizer=l2(0.0000001), activity_regularizer=l2(0.0000001))\n",
        "decoder_embeddings = decoder_embedding(decoder_inputs)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(5000, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "Seq2SeqModel = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='model_encoder_training')\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++ model for predictions +++++++++++++++++++++++++++++++++\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(1,))\n",
        "embedded = decoder_embedding(decoder_inputs)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(embedded, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "\n",
        "# +++++++++++++++++++++++++++++++++ Predict Class +++++++++++++++++++++++++++++++++\n",
        "class Predict():\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def create_response(self, question):\n",
        "        question = np.expand_dims(self.tokenizer.tokenize_sequence(clean_text(question)), axis=0)\n",
        "        result = self.predict_sentence(question)\n",
        "        return result\n",
        "\n",
        "    def predict_sentence(self, input_seq):\n",
        "        with tf.device('/cpu:0'):\n",
        "            states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = self.tokenizer.tokenizer.word_index['<sos>']\n",
        "            output_sentence = []\n",
        "\n",
        "            for _ in range(MAXLEN):\n",
        "                output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "                idx = np.argmax(output_tokens)\n",
        "\n",
        "                if self.tokenizer.tokenizer.index_word[idx] == '<eos>':\n",
        "                    break\n",
        "\n",
        "                output_sentence.append(idx)\n",
        "                target_seq[0, 0] = idx\n",
        "                states_value = [h, c]\n",
        "\n",
        "        return self.tokenizer.decode_sequence(output_sentence)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N4M9Gg-bX5fK",
        "outputId": "269f6a19-d070-4a0d-f314-aea82769f177"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "# loading tokenizer\n",
        "tokenizer.load_tokenizer('/mydrive/Maruf/Final_Everything/Arcadia/tokenizer-vocab_size-5000.pickle')\n",
        "\n",
        "# loading pretrained weight\n",
        "Seq2SeqModel.load_weights('/mydrive/Maruf/Final_Everything/Arcadia/seq2seq-weights-800-epochs-0.01-learning_rate.h5')\n",
        "\n",
        "predict = Predict(Seq2SeqModel, tokenizer)\n",
        "\n",
        "def chatwithbot(text):\n",
        "    return (predict.create_response(text))\n",
        "\n",
        "\n",
        "print(\"Hello\")\n",
        "\n",
        "while True:\n",
        "  text=input()\n",
        "  corrected_text=gf.correct(text)\n",
        "  \n",
        "  #print(type(corrected_text[0][0]))\n",
        "  #)\n",
        "  if ( text == \"name?\" or text == \"Name?\" or text == \"Your name?\" or text == \"your name?\" or text == \"Tell me your name\" or text ==\"tell me your name\"):\n",
        "    reply = \"My name is rdany but you can call me dany.\"\n",
        "    print(reply)\n",
        "  elif ( text == \"no one loves me\" or text == \"I feel I don't know\"):\n",
        "    reply = \"There's always someone for everyone. Don't do something stupid.\"\n",
        "    print(reply)\n",
        "  elif ( text == \"depressed\" or text == \"I am depressed\" or text == \"I want to kill myself\"):\n",
        "    reply = \"don't be depressed. Talk to someone professional or someone you love. There's always someone for everyone. Don't do something stupid.\"\n",
        "    print(reply)\n",
        "  elif ( text == corrected_text[0][0]):\n",
        "     reply=chatwithbot(corrected_text[0][0] )\n",
        "     print(reply)\n",
        "  else:\n",
        "      print(\"Did you mean: \")\n",
        "      print(corrected_text[0][0])\n",
        "      yn=input()\n",
        "      if(yn==\"yes\" or yn==\"Yes\"):\n",
        "          reply=chatwithbot(corrected_text[0][0])\n",
        "          print(reply)\n",
        "      \n",
        "  "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "tell me your name\n",
            "My name is rdany but you can call me dany.\n",
            "hi dany\n",
            "Did you mean: \n",
            "hi dany santa jo afos.\n",
            "no\n",
            "hello dany.\n",
            "hahaha\n",
            "I am depressed\n",
            "don't be depressed. Talk to someone professional or someone you love. There's always someone for everyone. Don't do something stupid.\n",
            "okay\n",
            "Did you mean: \n",
            "okay?\n",
            "no\n",
            "okay\n",
            "Did you mean: \n",
            "okay.\n",
            "yes\n",
            "and how it work\n",
            "don't know.\n",
            "oh so is a lot of the time hahaha is good or fun do you like it\n",
            "okay it was good talking to you.\n",
            "Did you mean: \n",
            "okay, it was good to talk to you.\n",
            "yes\n",
            "python\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-1704f1059039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mcorrected_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh1LkbEHdFee"
      },
      "source": [
        "print(type(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLhtB2pEU10W"
      },
      "source": [
        "sentences = [\n",
        "    'I like for walks', \n",
        "    'World is flat', \n",
        "    'Red a color', \n",
        "    'I wish my Computer was run faster.'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zoaXuQrU10Y"
      },
      "source": [
        "for sentence in sentences:\n",
        "    res = gf.correct(sentence)\n",
        "    print(res[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}